import os
import re
import sys
from urllib.parse import unquote
import aiohttp
from bs4 import BeautifulSoup
import importlib.util
from typing import List, Dict, Any, Tuple
import html

from flibusta_client import FlibustaClient
from constants import  SETTING_SEARCH_AREA_B, SETTING_SEARCH_AREA_BA #FLIBUSTA_BASE_URL

# –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω FB2
FB2_NAMESPACE = "http://www.gribuser.ru/xml/fictionbook/2.0"
XLINK_NAMESPACE = "http://www.w3.org/1999/xlink"

# –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –∏–º–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ XPath
NAMESPACES = {
    "fb": FB2_NAMESPACE,
    "xlink": XLINK_NAMESPACE,
}

# –ò–º—è –±–æ—Ç–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
BOT_USERNAME = os.getenv("BOT_USERNAME", "")


def format_size(size_in_bytes):
    units = ["B", "K", "M", "G", "T"]
    unit_index = 0
    while size_in_bytes >= 1024 and unit_index < len(units) - 1:
        size_in_bytes /= 1024
        unit_index += 1
    return f"{size_in_bytes:.1f}{units[unit_index]}"


def form_header_books(page, max_books, found_count, search_type='–∫–Ω–∏–≥', series_name=None, author_name=None,
                      search_area=SETTING_SEARCH_AREA_B):
    """ –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø–æ–∏—Å–∫–∞ –∫–Ω–∏–≥ """
    start = max_books * page + 1
    end = min(max_books * (page + 1), found_count)

    header = f"–ü–æ–∫–∞–∑—ã–≤–∞—é —Å {start} –ø–æ {end} –∏–∑ {found_count} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö {search_type}"

    header += f" –≤ —Å–µ—Ä–∏–∏ '{series_name}'" if series_name else ""
    header += f" –∞–≤—Ç–æ—Ä–∞ '{author_name}'" if author_name else ""
    header += " –ø–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫–Ω–∏–≥–∏" if search_area == SETTING_SEARCH_AREA_BA else ""

    return header


def get_platform_recommendations() -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
    (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Ç–∞–∫ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–ª–∞—Ç—Ñ–æ—Ä–º—É —Å–ª–æ–∂–Ω–æ)
    """
    return """
üì± <b>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —á–∏—Ç–∞–ª–∫–∏ –¥–ª—è –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º:</b>
<u>–î–ª—è Android:</u>
‚Ä¢ üìñ <a href="https://play.google.com/store/apps/details?id=org.readera">ReadEra</a> - –ª—É—á—à–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è
‚Ä¢ üìö <a href="https://play.google.com/store/apps/details?id=com.flyersoft.moonreader">Moon+ Reader</a>
‚Ä¢ üî• <a href="https://play.google.com/store/apps/details?id=com.amazon.kindle">Kindle</a>

<u>–î–ª—è iOS:</u>
‚Ä¢ üìñ <a href="https://apps.apple.com/ru/app/readera-—á–∏—Ç–∞–ª–∫–∞-–∫–Ω–∏–≥-pdf/id1441824222">ReadEra</a>
‚Ä¢ üìö <a href="https://apps.apple.com/ru/app/kybook-3-ebook-reader/id1259787028">KyBook 3</a>
‚Ä¢ üî• <a href="https://apps.apple.com/ru/app/amazon-kindle/id302584613">Kindle</a>

<u>–î–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞:</u>
‚Ä¢ üìö <a href="https://www.calibre-ebook.com/">Calibre</a> (Windows/Mac/Linux)
‚Ä¢ üìò <a href="https://apps.apple.com/ru/app/apple-books/id364709193">Apple Books</a> (Mac)
‚Ä¢ üìñ <a href="https://www.amazon.com/b?node=16571048011">Kindle</a> (–≤—Å–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã)
"""


# ===== –°–õ–£–ñ–ï–ë–ù–´–ï –§–£–ù–ö–¶–ò–ò =====

# async def download_book_with_filename(url: str):
#     """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–Ω–∏–≥—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ + –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞"""
#     try:
#         async with aiohttp.ClientSession() as session:
#             async with session.get(url) as response:
#                 if response.status == 200:
#                     book_data = await response.read()
#                     filename = None
#
#                     content_disposition = response.headers.get('Content-Disposition', '')
#                     if content_disposition:
#                         filename_match = re.search(r'filename[^;=\n]*=([\'"]?)([^\'"\n]+)\1', content_disposition,
#                                                    re.IGNORECASE)
#                         if filename_match:
#                             filename = unquote(filename_match.group(2))
#
#                     return book_data, filename
#                 return None, None
#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫–Ω–∏–≥–∏: {e}")
#         return None, None


async def upload_to_tmpfiles(file, file_name: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –Ω–∞ tmpfiles.org –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
    try:
        async with aiohttp.ClientSession() as session:
            form_data = aiohttp.FormData()
            form_data.add_field('file', file, filename=file_name)
            params = {'duration': '15m'}

            async with session.post(
                    'https://tmpfiles.org/api/v1/upload',
                    data=form_data,
                    params=params
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result['data']['url']
                return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return None


# ===== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –î–õ–Ø –ì–†–£–ü–ü–û–í–û–ì–û –ß–ê–¢–ê =====

def is_message_for_bot(message_text, bot_username):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫ –±–æ—Ç—É"""
    if not bot_username:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –±–æ—Ç–∞ –≤ –Ω–∞—á–∞–ª–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    return message_text.startswith(f'@{bot_username}')


def extract_clean_query(message_text, bot_username):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    if not bot_username:
        return message_text.strip()

    # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –±–æ—Ç–∞
    clean_text = message_text.replace(f'@{bot_username}', '').strip()

    return clean_text


# ===== –ó–ê–ì–†–£–ó–ö–ê –ù–û–í–û–°–¢–ï–ô –ò–ó PYTHON –§–ê–ô–õ–ê =====

async def load_bot_news(file_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –±–æ—Ç–∞ –∏–∑ Python —Ñ–∞–π–ª–∞"""
    try:
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª—è–µ–º –º–æ–¥—É–ª—å –∏–∑ –∫—ç—à–∞, –µ—Å–ª–∏ –æ–Ω —É–∂–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω
        if "bot_news" in sys.modules:
            del sys.modules["bot_news"]

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        spec = importlib.util.spec_from_file_location("bot_news", file_path)
        news_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(news_module)

        news = getattr(news_module, 'BOT_NEWS', [])
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {file_path}")
        return news

    except FileNotFoundError:
        print(f"–§–∞–π–ª –Ω–æ–≤–æ—Å—Ç–µ–π {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {file_path}: {e}")
        return []


async def get_latest_news(file_path: str, count: int = 3) -> List[Dict[str, Any]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ count –Ω–æ–≤–æ—Å—Ç–µ–π"""
    all_news = await load_bot_news(file_path)
    return all_news[-count:] if all_news else []


# ===== –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –í–´–í–û–î–ê =====
def truncate_text(text, no_more_len, stop_sep) -> str:
    if len(text) <= no_more_len:
        return text
    else:
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ no_more_len —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∏—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π stop —Å–∏–º–≤–æ–ª
        truncated = text[:no_more_len]
        last_stop_char = truncated.rfind(stop_sep)
        if last_stop_char != -1:
            return truncated[:last_stop_char] + "..."
        else:
            # –ï—Å–ª–∏ –∑–∞–ø—è—Ç—ã—Ö –Ω–µ—Ç ‚Äî –∑–Ω–∞—á–∏—Ç, –æ–¥–∏–Ω –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
             return truncated + "..."


def format_links_from_flat_string(url_routine, flat_str: str, max_num_elem: int) -> Tuple[str, bool]:
    if not flat_str:
        return "", False

    parts = [part.strip() for part in flat_str.split(',') if part.strip()]
    orig_len = len(parts)
    parts = parts[:max_num_elem]
    trunc_len = len(parts)

    # –ï—Å–ª–∏ –Ω–µ—á—ë—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ‚Äî –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–µ–ø–∞—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if len(parts) % 2 != 0:
        parts = parts[:-1]

    links = []
    for i in range(0, len(parts), 2):
        try:
            elem_id = int(parts[i])
            elem_name = parts[i + 1]
            url = url_routine(elem_id)
            links.append(f"<a href='{url}'>{elem_name}</a>")
        except (ValueError, IndexError):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä—ã
            continue

    return ", ".join(links), orig_len != trunc_len

def format_book_info(book_info):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è"""
    text = f"üìö <b><a href='{FlibustaClient.get_book_url(book_info['bookid'])}'>{book_info['title']}</a></b>\n"
    # authors = book_info['authors'][:300] + ("..." if len(book_info['authors']) > 300 else "")
    author_links, is_truncated = format_links_from_flat_string(FlibustaClient.get_author_url, book_info['authors'], 20)
    text += f"\nüë§ <b>–ê–≤—Ç–æ—Ä(—ã):</b> {(author_links + (',...' if is_truncated else '')) or '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}"
    year = book_info['year']
    series = book_info['series']
    genre_links, is_truncated = format_links_from_flat_string(FlibustaClient.get_genre_url, book_info['genres'], 10)
    lang = book_info['lang']
    pages = book_info['pages']
    rate = book_info['rate']
    # book_id = book_info['bookid']
    series_id = book_info['seqid']
    if year and year != 0:
        text += f"\nüìÖ <b>–ì–æ–¥:</b> {year}"
    if series:
        text += f"\nüìñ <b>–°–µ—Ä–∏—è:</b> <a href='{FlibustaClient.get_series_url(series_id)}'>{series}</a>"
    if genre_links:
        text += f"\nüìë <b>–ñ–∞–Ω—Ä(—ã):</b> {(genre_links + (',...' if is_truncated else '')) or '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}"
    if lang:
        text += f"\nüó£Ô∏è <b>–Ø–∑—ã–∫:</b> {lang}"
    if pages:
        text += f"\nüìÉ <b>–°—Ç—Ä–∞–Ω–∏—Ü:</b> {pages}"
    size = format_size(book_info['size'])
    text += f"\nüì¶ <b>–†–∞–∑–º–µ—Ä:</b> {size}"
    if rate:
        text += f"\n‚≠ê <b>–†–µ–π—Ç–∏–Ω–≥:</b> {rate:.1f}"
    # if book_id:
    #     text += f"\nüîë <b>ID:</b> <a href='{FlibustaClient.get_book_url(book_id)}'>{book_id}</a>"
    return text


def format_book_details(book_details):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ"""
    text = f"üìñ <b>–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ:</b> {book_details.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n\n"
    if book_details.get('annotation'):
        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º–∞
        clean_annotation = clean_html_tags(book_details['annotation'])
        # text += f"{clean_annotation[:4000]}" + ("..." if len(clean_annotation) > 4000 else "")
        text += clean_annotation

    return truncate_text(text, 4000, '.')


def format_author_info(author_info):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–µ"""
    text = f"üë§ <b>–û–± –∞–≤—Ç–æ—Ä–µ:</b> <a href='{FlibustaClient.get_author_url(author_info['author_id'])}'>{author_info['name']}</a>\n\n"
    if author_info.get('biography'):
        clean_bio = clean_html_tags(author_info['biography'])
        # text += f"{clean_bio[:4000]}" + ("..." if len(clean_bio) > 4000 else "")
        text += clean_bio

    return truncate_text(text, 4000, '.')


def format_book_reviews(reviews):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–∑—ã–≤—ã –æ –∫–Ω–∏–≥–µ"""
    text = "üí¨ <b>–û—Ç–∑—ã–≤—ã –æ –∫–Ω–∏–≥–µ:</b>\n\n"

    for name, time, review_text in reviews[:50]:
        reviewer = f"üë§ <b>{name}</b> ({time})\n"
        clean_review = clean_html_tags(review_text)
        clean_review_trunc = f"{clean_review[:1000]}" + ("..." if len(clean_review) > 1000 else "") + "\n"
        if len(text + reviewer + clean_review_trunc) > 4000:
            break
        text += reviewer
        text += clean_review_trunc

    return text

def clean_html_tags(text):
    """–£–¥–∞–ª—è–µ–º html-—Ç–µ–≥–∏ –∏ –æ—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ –º—É—Å–æ—Ä–∞"""
    clean_text = text
    clean_text = re.sub(r'<br\s*/?>', '\n', clean_text)  # <br> ‚Üí –ø–µ—Ä–µ–Ω–æ—Å
    clean_text = re.sub(r'</?p[^>]*>', '\n', clean_text)  # <p> ‚Üí –ø–µ—Ä–µ–Ω–æ—Å
    clean_text = re.sub(r'<[^<]+?>', '', clean_text)
    clean_text = re.sub(r'\[[^\]]*?\]', '', clean_text)  # –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
    clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)
    clean_text = html.escape(clean_text)
    clean_text = clean_text.strip()
    return clean_text

# async def get_cover_url(book_id: str):
#     """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –æ–±–ª–æ–∂–∫–∏ —á–µ—Ä–µ–∑ BeautifulSoup"""
#     try:
#         url = f"{FLIBUSTA_BASE_URL}/b/{book_id}"
#         async with aiohttp.ClientSession() as session:
#             async with session.get(url) as response:
#                 if response.status == 200:
#                     html_resp = await response.text()
#                     # print(f"DEBUG: html_resp = {html_resp}")
#                     soup = BeautifulSoup(html_resp, 'html.parser')
#                     # –ò—â–µ–º –æ–±–ª–æ–∂–∫—É –ø–æ title –∏–ª–∏ alt
#                     cover_img = soup.find('img', {'title': 'Cover image'})
#                     if not cover_img:
#                         cover_img = soup.find('img', {'alt': 'Cover image'})
#
#                     if cover_img and cover_img.get('src'):
#                         cover_url = cover_img['src']
#                         if not cover_url.startswith('http'):
#                             cover_url = f"{FLIBUSTA_BASE_URL}{cover_url}"
#                         return cover_url
#         return None
#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–ª–æ–∂–∫–∏: {e}")
#         return None
